{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2: Neural Machine Translation in the wild\n",
    "In the third homework you are supposed to get the best translation you can for the EN-RU translation task.\n",
    "\n",
    "Basic approach using RNNs as encoder and decoder is implemented for you. \n",
    "\n",
    "Your ultimate task is to use the techniques we've covered, e.g.\n",
    "\n",
    "* Optimization enhancements (e.g. learning rate decay)\n",
    "\n",
    "* CNN encoder (with or without positional encoding)\n",
    "\n",
    "* attention/self-attention mechanism\n",
    "\n",
    "* pretraining the language model\n",
    "\n",
    "* [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt)\n",
    "\n",
    "* or just fine-tunning BERT ;)\n",
    "\n",
    "to improve the translation quality. \n",
    "\n",
    "__Please use at least three different approaches/models and compare them (translation quality/complexity/training and evaluation time).__\n",
    "\n",
    "Write down some summary on your experiments and illustrate it with convergence plots/metrics and your thoughts. Just like you would approach a real problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to install the libraries below. Do it in the desired environment\n",
    "# if you are working locally.\n",
    "\n",
    "# ! pip  install subword-nmt\n",
    "# ! pip install nltk\n",
    "# ! pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to YSDA NLP course team for the data\n",
    "# (who thanks tilda and deephack teams for the data in their turn)\n",
    "\n",
    "import os\n",
    "path_do_data = '../../datasets/Machine_translation_EN_RU/data.txt'\n",
    "if not os.path.exists(path_do_data):\n",
    "    print(\"Dataset not found locally. Downloading from github. Loading special files as well\")\n",
    "    !wget https://raw.githubusercontent.com/neychev/made_nlp_course/spring2021/datasets/Machine_translation_EN_RU/data.txt -nc\n",
    "    path_do_data = './data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./utils.py'):\n",
    "    print(\"utils file not found locally. Downloading from github.\")\n",
    "    !wget https://raw.githubusercontent.com/neychev/made_nlp_course/spring2021/homeworks/Lab02_NMT/utils.py -nc\n",
    "\n",
    "if not os.path.exists('./my_network.py'):\n",
    "    print(\"network file not found locally. Downloading from github.\")\n",
    "    !wget https://raw.githubusercontent.com/neychev/made_nlp_course/spring2021/homeworks/Lab02_NMT/my_network.py -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "# TODO check torchtext version\n",
    "if torchtext.__version__ == '0.8.1':\n",
    "    from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "WANDB_GLOBAL = dict(\n",
    "    entity='ermekaitygulov',\n",
    "    group='Baseline',\n",
    "    anonymous='allow',\n",
    "    project='NLP-LAB2',\n",
    "    reinit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main part\n",
    "__Here comes the preprocessing. Do not hesitate to use BPE or more complex preprocessing ;)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_W = WordPunctTokenizer()\n",
    "def tokenize(x, tokenizer=tokenizer_W):\n",
    "    return tokenizer.tokenize(x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cherepaha/miniconda/envs/py37/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/cherepaha/miniconda/envs/py37/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/cherepaha/miniconda/envs/py37/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SRC = Field(tokenize=tokenize,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize=tokenize,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "dataset = TabularDataset(\n",
    "    path=path_do_data,\n",
    "    format='tsv',\n",
    "    fields=[('trg', TRG), ('src', SRC)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = dataset.split(split_ratio=[0.8, 0.15, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 40000\n",
      "Number of validation examples: 2500\n",
      "Number of testing examples: 7500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (ru) vocabulary: 5848\n",
      "Unique tokens in target (en) vocabulary: 4254\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 5)\n",
    "TRG.build_vocab(train_data, min_freq = 5)\n",
    "print(f\"Unique tokens in source (ru) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the length distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length distribution in Test data\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_length = map(len, [vars(x)['src'] for x in test_data.examples])\n",
    "trg_length = map(len, [vars(x)['trg'] for x in test_data.examples])\n",
    "\n",
    "print('Length distribution in Test data')\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(src_length), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(trg_length), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model side\n",
    "__Here comes simple pipeline of NMT model learning. It almost copies the week03 practice__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cherepaha/miniconda/envs/py37/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "def _len_sort_key(x):\n",
    "    return len(x.src)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_key=_len_sort_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_network\n",
    "Encoder = my_network.Encoder\n",
    "Decoder = my_network.Decoder\n",
    "Seq2Seq = my_network.Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.11<br/>\n                Syncing run <strong style=\"color:#cdcd00\">less_hd_2ifqef4p</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/ermekaitygulov/NLP-LAB2\" target=\"_blank\">https://wandb.ai/ermekaitygulov/NLP-LAB2</a><br/>\n                Run page: <a href=\"https://wandb.ai/ermekaitygulov/NLP-LAB2/runs/2gai4wi5\" target=\"_blank\">https://wandb.ai/ermekaitygulov/NLP-LAB2/runs/2gai4wi5</a><br/>\n                Run data is saved locally in <code>/home/cherepaha/Projects/made/2sem/made_nlp_course/homeworks/Lab02_NMT/wandb/run-20210512_153817-2gai4wi5</code><br/><br/>\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[<wandb.wandb_torch.TorchGraph at 0x7f5ae62b5cc0>]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "train_step = 0\n",
    "val_step = 0\n",
    "best_valid_loss = float('inf')\n",
    "model_name = 'less_hd'\n",
    "model_config = {}\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "\n",
    "name = f'{model_name}_{wandb.util.generate_id()}'\n",
    "if wandb.run:\n",
    "    wandb.finish()\n",
    "wandb.init(name=name, config=model_config, **WANDB_GLOBAL)\n",
    "wandb.watch(model, criterion, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,784,734 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model.apply(init_weights)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:46<00:00, 13.34it/s, train_loss=4.53]\n",
      "100%|██████████| 40/40 [00:00<00:00, 57.52it/s, train_loss=5.28]\n",
      "  0%|          | 1/625 [00:00<01:20,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 47s\n",
      "\tTrain Loss: 5.091 | Train PPL: 162.607\n",
      "\t Val. Loss: 5.194 |  Val. PPL: 180.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:47<00:00, 13.13it/s, train_loss=4.01]\n",
      "100%|██████████| 40/40 [00:00<00:00, 58.31it/s, train_loss=5.28]\n",
      "  0%|          | 2/625 [00:00<00:51, 12.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 48s\n",
      "\tTrain Loss: 4.190 | Train PPL:  65.993\n",
      "\t Val. Loss: 5.100 |  Val. PPL: 163.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:48<00:00, 12.88it/s, train_loss=3.76]\n",
      "100%|██████████| 40/40 [00:00<00:00, 56.91it/s, train_loss=5.23]\n",
      "  0%|          | 1/625 [00:00<01:12,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 49s\n",
      "\tTrain Loss: 3.823 | Train PPL:  45.741\n",
      "\t Val. Loss: 4.894 |  Val. PPL: 133.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:47<00:00, 13.05it/s, train_loss=3.49]\n",
      "100%|██████████| 40/40 [00:00<00:00, 56.87it/s, train_loss=5.16]\n",
      "  0%|          | 1/625 [00:00<01:09,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 48s\n",
      "\tTrain Loss: 3.564 | Train PPL:  35.319\n",
      "\t Val. Loss: 4.791 |  Val. PPL: 120.364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:48<00:00, 13.02it/s, train_loss=3.44]\n",
      "100%|██████████| 40/40 [00:00<00:00, 54.31it/s, train_loss=5.13]\n",
      "  0%|          | 2/625 [00:00<00:54, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 48s\n",
      "\tTrain Loss: 3.397 | Train PPL:  29.882\n",
      "\t Val. Loss: 4.715 |  Val. PPL: 111.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:48<00:00, 12.91it/s, train_loss=3.32]\n",
      "100%|██████████| 40/40 [00:00<00:00, 55.84it/s, train_loss=5.04]\n",
      "  0%|          | 1/625 [00:00<01:10,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0m 49s\n",
      "\tTrain Loss: 3.261 | Train PPL:  26.065\n",
      "\t Val. Loss: 4.589 |  Val. PPL:  98.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:48<00:00, 12.87it/s, train_loss=3.38]\n",
      "100%|██████████| 40/40 [00:00<00:00, 58.01it/s, train_loss=5.01]\n",
      "  0%|          | 1/625 [00:00<01:12,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0m 49s\n",
      "\tTrain Loss: 3.165 | Train PPL:  23.690\n",
      "\t Val. Loss: 4.524 |  Val. PPL:  92.220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:48<00:00, 12.96it/s, train_loss=3.07]\n",
      "100%|██████████| 40/40 [00:00<00:00, 57.70it/s, train_loss=5.1] \n",
      "  0%|          | 1/625 [00:00<01:08,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0m 48s\n",
      "\tTrain Loss: 3.079 | Train PPL:  21.741\n",
      "\t Val. Loss: 4.615 |  Val. PPL: 101.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:48<00:00, 12.80it/s, train_loss=2.99]\n",
      "100%|██████████| 40/40 [00:00<00:00, 56.89it/s, train_loss=5.06]\n",
      "  0%|          | 1/625 [00:00<01:08,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0m 49s\n",
      "\tTrain Loss: 2.997 | Train PPL:  20.030\n",
      "\t Val. Loss: 4.510 |  Val. PPL:  90.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:47<00:00, 13.04it/s, train_loss=3.06]\n",
      "100%|██████████| 40/40 [00:00<00:00, 57.14it/s, train_loss=5.04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0m 48s\n",
      "\tTrain Loss: 2.931 | Train PPL:  18.753\n",
      "\t Val. Loss: 4.483 |  Val. PPL:  88.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_step = train(\n",
    "        model,\n",
    "        train_iterator,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        CLIP,\n",
    "        train_step)\n",
    "\n",
    "    valid_loss, val_step = evaluate(\n",
    "        model,\n",
    "        valid_iterator,\n",
    "        criterion,\n",
    "        val_step,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [00:02, 56.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 28944<br/>Program ended successfully."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "557181f83c4640ed8e04d7bf075f6e81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/home/cherepaha/Projects/made/2sem/made_nlp_course/homeworks/Lab02_NMT/wandb/run-20210512_153817-2gai4wi5/logs/debug.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/home/cherepaha/Projects/made/2sem/made_nlp_course/homeworks/Lab02_NMT/wandb/run-20210512_153817-2gai4wi5/logs/debug-internal.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>train_loss</td><td>3.0559</td></tr><tr><td>train_step</td><td>6244</td></tr><tr><td>train_speed(batch/sec)</td><td>12.5405</td></tr><tr><td>_step</td><td>660</td></tr><tr><td>_runtime</td><td>535</td></tr><tr><td>_timestamp</td><td>1620823632</td></tr><tr><td>val_loss</td><td>5.03875</td></tr><tr><td>val_step</td><td>399</td></tr></table>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>train_loss</td><td>██▇▆▆▅▅▄▄▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁▂▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_speed(batch/sec)</td><td>█▅▇▇▇▅▅▇▄▅▅▅▆▆▆▁▅▅▆▄▅▄▅▄▃▅▄▄▆▄▆▅▆█▅▆▅▄▅█</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▇█▇▆▇█▄▅▆█▄▅▆▇▃▄▅▇▂▃▅▇▂▃▄▇▂▃▅▇▁▃▅▇▁▃▄▇</td></tr><tr><td>val_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">less_hd_2ifqef4p</strong>: <a href=\"https://wandb.ai/ermekaitygulov/NLP-LAB2/runs/2gai4wi5\" target=\"_blank\">https://wandb.ai/ermekaitygulov/NLP-LAB2/runs/2gai4wi5</a><br/>\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(model, test_iterator, TRG.vocab, train_step)\n",
    "FINISH = True\n",
    "if FINISH:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's take a look at our network quality__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: all rooms come with a tv .\n",
      "Generated: each room is fitted with a tv .\n",
      "\n",
      "Original: there is a 24 - hour front desk at the property .\n",
      "Generated: a 24 - hour front desk .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_iterator))\n",
    "for idx in [1,2]:\n",
    "    src = batch.src[:, idx:idx+1]\n",
    "    trg = batch.trg[:, idx:idx+1]\n",
    "    generate_translation(src, trg, model, TRG.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline solution BLEU score is quite low. Try to achieve at least __24__ BLEU on the test set. \n",
    "The checkpoints are:\n",
    "\n",
    "* __22__ - minimal score to submit the homework, 30% of points\n",
    "\n",
    "* __27__ - good score, 70% of points\n",
    "\n",
    "* __29__ - excellent score, 100% of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}