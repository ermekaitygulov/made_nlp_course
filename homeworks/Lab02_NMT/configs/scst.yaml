data:
  train_size: 0.8
  val_size: 0.15
  test_size: 0.05
  batch_size: 64
  path: '../../datasets/Machine_translation_EN_RU/data.txt'
  word_min_freq: 5
pretrain:
  epoch: 5
  opt_class: Adam
  scheduler_class: OneCycleLR
  scheduler_params:
    max_lr: 0.001
    steps_per_epoch: 2649
    epochs: 5
  grad_clip: 1
train:
  epoch: 15
  opt_class: Adam
  scheduler_class: OneCycleLR
  scheduler_params:
    max_lr: 0.001
    steps_per_epoch: 2649
    epochs: 15
  grad_clip: 1
  teacher_enforce:
    ratio_start: 0.0
    ratio_growth: 0.1
    ratio_max: 0.5
scst:
  epoch: 5
  opt_class: Adam
  opt_params:
    lr: 0.0001
  grad_clip: 1
  entropy_weight: 0.001
model_path: model_save/pretrain_baseline_2jxtl023/final-model.pt
model:
  name: lstm_teacher
  params:
    hid_dim: &hid_dim 512
    n_layers: &n_layers 2
    teacher_forcing_ratio: 0
    encoder:
      emb_dim: 512
      dropout: 0.2
      hid_dim: *hid_dim
      n_layers: *n_layers
    decoder:
      emb_dim: 512
      dropout: 0.2
      hid_dim: *hid_dim
      n_layers: *n_layers
