data:
  train_size: 0.5
  val_size: 0.4
  test_size: 0.1
  batch_size: 64
  path: '../../datasets/Machine_translation_EN_RU/data.txt'
  word_min_freq: 5
pretrain:
  epoch: 0
  opt_class: Adam
  scheduler_class: CosineAnnealingWarmRestarts
  scheduler_params:
    T_0: 625
    T_mult: 2
  grad_clip: 1
train:
  epoch: 10
  opt_class: Adam
  scheduler_class: CosineAnnealingWarmRestarts
  scheduler_params:
    T_0: 625
    T_mult: 2
  grad_clip: 1
  teacher_enforce:
    ratio_start: 0.0
    ratio_growth: 0.1
    ratio_max: 0.5
sgd_train:
  epoch: 0
  opt_class: SGD
  opt_params:
    lr: 0.0001
    momentum: 0.9
    nesterov: True
  scheduler_class: CosineAnnealingWarmRestarts
  scheduler_params:
    T_0: 2500
    T_mult: 2
  grad_clip: 1
  teacher_enforce:
    ratio_start: 0.5
    ratio_growth: 0.1
    ratio_max: 0.8
#model_path: 'model_save/pretrain_baseline_ogo7yye5/main_stage-model.pt'
model:
  name: transformer
  params:
    emb_dim: 256
    encoder:
      layer_number: 3
      dim_feedforward: 1024
      dropout: 0.2
      nhead: 8
    decoder:
      layer_number: 3
      dim_feedforward: 1024
      dropout: 0.2
      nhead: 8
    pos_encoder:
      dropout: 0.1
#    hid_dim: &hid_dim 256
#    n_layers: &n_layers 2
#    teacher_forcing_ratio: 0
#    encoder:
#      emb_dim: 512
#      dropout: 0.5
#      hid_dim: *hid_dim
#      n_layers: *n_layers
#    decoder:
#      emb_dim: 512
#      dropout: 0.5
#      hid_dim: *hid_dim
#      n_layers: *n_layers
