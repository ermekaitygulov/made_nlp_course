src_bpe: 'configs/ru_bpe_2000'
trg_bpe: 'configs/en_bpe_2000'
data:
  train_size: 0.8
  val_size: 0.15
  test_size: 0.05
  batch_size: 64
  path: '../../datasets/Machine_translation_EN_RU/data.txt'
  word_min_freq: 5
pretrain:
  epoch: 5
  opt_class: Adam
  scheduler_class: OneCycleLR
  scheduler_params:
    max_lr: 0.001
    steps_per_epoch: 625
    epochs: 5
  grad_clip: 1
train:
  epoch: 15
  opt_class: Adam
  scheduler_class: OneCycleLR
  scheduler_params:
    max_lr: 0.001
    steps_per_epoch: 625
    epochs: 15
  grad_clip: 1
  teacher_enforce:
    ratio_start: 0.0
    ratio_growth: 0.1
    ratio_max: 0.5
sgd_train:
  epoch: 0
  opt_class: SGD
  opt_params:
    lr: 0.0001
    momentum: 0.9
    nesterov: True
  scheduler_class: CosineAnnealingWarmRestarts
  scheduler_params:
    T_0: 2500
    T_mult: 2
  grad_clip: 1
  teacher_enforce:
    ratio_start: 0.5
    ratio_growth: 0.1
    ratio_max: 0.8
#model_path: 'main_stage-model.pt'
model:
  name: lstm_teacher
  params:
    hid_dim: &hid_dim 512
    n_layers: &n_layers 2
    teacher_forcing_ratio: 0
    encoder:
      emb_dim: 512
      dropout: 0.2
      hid_dim: *hid_dim
      n_layers: *n_layers
    decoder:
      emb_dim: 512
      dropout: 0.2
      hid_dim: *hid_dim
      n_layers: *n_layers
